{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_learn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imransalam/basic-tf-intro-notebooks/blob/master/tf_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dd-jrA-KvEfk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **some tensorflow stuff....**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9X3u3UXkvkbx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Please Go to Runtime type and change environment to Python3.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "You can change the hardware accelerator to GPU too (if your luck is good enough)"
      ]
    },
    {
      "metadata": {
        "id": "Fe4ViPhuvbtu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWDxbeDci1_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Constants**"
      ]
    },
    {
      "metadata": {
        "id": "Oa0kaU-tveuA",
        "colab_type": "code",
        "outputId": "8aa15daa-06ab-4418-d111-a0bc0cf2ae1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Some basic Mathematics Operations Over Constants\n",
        "'''\n",
        "\n",
        "# CREATE GRAPH\n",
        "basic_addition_graph = tf.Graph()\n",
        "with basic_addition_graph.as_default():\n",
        "    a = tf.constant(value=[ 2, 5, 7], \n",
        "                    name=\"a\", \n",
        "                    dtype=tf.int8, \n",
        "                    shape=(3,), \n",
        "                    verify_shape=True)\n",
        "    \n",
        "    b = tf.constant(value=[ 1, -2, -4], \n",
        "                    name=\"b\", \n",
        "                    dtype=tf.int8, \n",
        "                    shape=(3,), \n",
        "                    verify_shape=True)\n",
        "    \n",
        "    c = tf.constant(value=[[ -4, -1, 2], [ -4, -1, 2]], \n",
        "                name=\"c\", \n",
        "                dtype=tf.int8, \n",
        "                shape=(2,3), \n",
        "                verify_shape=True)\n",
        "    \n",
        "    \n",
        "    x = tf.add(a, b)\n",
        "    y = tf.multiply(x , c) # BROADCASTING, (3, ) X (2,3)\n",
        "    \n",
        "\n",
        "# CREATE A SESSION TO RUN THE GRAPH\n",
        "with tf.Session(graph=basic_addition_graph) as sess:\n",
        "    \n",
        "    # writer = tf.summary.FileWriter('basic_addition_graph', sess.graph)\n",
        "    # tensorboard --logdir=\"basic_addition_graph\" --port 8000\n",
        "    \n",
        "    print(sess.run([x, y])) # Providing No feed_dict, No Placeholder in the graph"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([3, 3, 3], dtype=int8), array([[-12,  -3,   6],\n",
            "       [-12,  -3,   6]], dtype=int8)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tX0vouVXjFW8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Placeholders**"
      ]
    },
    {
      "metadata": {
        "id": "pGkFDkHSy1iO",
        "colab_type": "code",
        "outputId": "f9c695a7-c174-4b87-c519-42e7221dba8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Some basic Mathematics Operations Over Placeholders\n",
        "'''\n",
        "\n",
        "# CREATE GRAPH\n",
        "basic_addition_graph = tf.Graph()\n",
        "with basic_addition_graph.as_default():\n",
        "    a = tf.placeholder(name=\"a\", \n",
        "                    dtype=tf.int8, \n",
        "                    shape=(3,))\n",
        "    \n",
        "    b = tf.placeholder(name=\"b\", \n",
        "                    dtype=tf.int8, \n",
        "                    shape=(3,))\n",
        "    \n",
        "    c = tf.constant(value=[[ -4, -1, 2], [ -4, -1, 2]], \n",
        "                name=\"c\", \n",
        "                dtype=tf.int8, \n",
        "                shape=(2,3), \n",
        "                verify_shape=True)\n",
        "    \n",
        "    \n",
        "    x = tf.add(a, b)\n",
        "    y = tf.multiply(x , c) # BROADCASTING, (3, ) X (2,3)\n",
        "    \n",
        "\n",
        "# CREATE A SESSION TO RUN THE GRAPH\n",
        "with tf.Session(graph=basic_addition_graph) as sess:\n",
        "    \n",
        "    # writer = tf.summary.FileWriter('basic_addition_graph', sess.graph)\n",
        "    # tensorboard --logdir=\"basic_addition_graph\" --port 8000\n",
        "    \n",
        "    print( sess.run([x, y], feed_dict={a:[ 2, 5, 7], b:[ 1, -2, -4]}) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([3, 3, 3], dtype=int8), array([[-12,  -3,   6],\n",
            "       [-12,  -3,   6]], dtype=int8)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q2oT0iPaklAI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Variables**"
      ]
    },
    {
      "metadata": {
        "id": "1HH9n06Ikoa7",
        "colab_type": "code",
        "outputId": "818b090b-4569-4197-86dd-a0feda580d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Some basic Mathematics Operations Over Placeholders and Variables\n",
        "'''\n",
        "\n",
        "# WHY VARIABLES?\n",
        "# WE WANT A MATRIX/VECTOR THAT CAN BE OPTIMIZED.\n",
        "\n",
        "\n",
        "# DIFFERENCE IN VARIABLES TYPE THAN PLACEHOLDERS OR CONSTANTS?\n",
        "# CONSTANTS AND PLACEHOLDERS ARE OPS. THEY ARE SIMPLY SCALARS / VECTORS / MATRICES / TENSORS. \n",
        "# VARIABLE ON THE OTHER HAND IS A CLASS. IT HAS ITS OWN METHODS DEFINED INSIDE IT.\n",
        "# HENCE YOU WOULD NOTICE THE INITIAL OF VARIABLE IS IN CAPITAL.\n",
        "\n",
        "# CREATE GRAPH\n",
        "basic_graph = tf.Graph()\n",
        "with basic_graph.as_default():\n",
        "    a = tf.placeholder(name=\"a\",\n",
        "                    dtype=tf.float32,\n",
        "                    shape=(None,5))\n",
        "    \n",
        "    \n",
        "    W = tf.Variable(tf.random_normal((5, 3)), name=\"W\") # INITIALIZE IT WITH RANDOM\n",
        "    b = tf.Variable(tf.random_normal((3,)), name=\"b\") # INITIALIZE IT WITH RANDOM\n",
        "    \n",
        "    y = tf.add(tf.matmul(a, W), b)\n",
        "    \n",
        "\n",
        "# CREATE A SESSION TO RUN THE GRAPH\n",
        "with tf.Session(graph=basic_graph) as sess:\n",
        "    \n",
        "    # writer = tf.summary.FileWriter('basic_addition_graph', sess.graph)\n",
        "    # tensorboard --logdir=\"basic_addition_graph\" --port 8000\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    print( sess.run(y, feed_dict={a: np.random.uniform(size=(500, 5)) }) )\n",
        "    print('-' * 20)\n",
        "    print(b.eval())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.739159   1.1435039  1.5349289 ]\n",
            " [1.1710117  1.4256032  1.5753393 ]\n",
            " [2.264703   0.75571954 1.3597436 ]\n",
            " ...\n",
            " [2.8382485  0.7648671  1.9334944 ]\n",
            " [2.087041   1.2666237  2.5543098 ]\n",
            " [1.6948155  2.1569772  2.3694232 ]]\n",
            "--------------------\n",
            "[ 1.5039895  -0.19186825  0.3403521 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FvOU2I1m3nkM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Start Some Training**"
      ]
    },
    {
      "metadata": {
        "id": "M_WbpUOm3wJB",
        "colab_type": "code",
        "outputId": "98d71bb6-6774-4fba-9286-fd0557bcdda9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5417
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Training Over Boston Housing. Its a simple Linear regression :/\n",
        "'''\n",
        "\n",
        "# LOAD DATASET\n",
        "dataset = tf.keras.datasets.boston_housing\n",
        "(x_train, y_train), (x_val, y_val) = dataset.load_data()\n",
        "\n",
        "# CONFIGURE\n",
        "lr = 1e-7\n",
        "show_every = 100\n",
        "epochs = 10000\n",
        "\n",
        "# CREATE GRAPH\n",
        "basic_linear_regression_graph = tf.Graph()\n",
        "with basic_linear_regression_graph.as_default():\n",
        "    \n",
        "    x = tf.placeholder(name=\"x\",\n",
        "                    dtype=tf.float64)\n",
        "    \n",
        "    y = tf.placeholder(name=\"y\",\n",
        "                    dtype=tf.float64)\n",
        "    \n",
        "    W = tf.Variable(tf.truncated_normal([13, 1], mean=0.0, stddev=1.0, dtype=tf.float64), name=\"W\") # INITIALIZE IT WITH RANDOM\n",
        "    b = tf.Variable(tf.zeros(1, dtype = tf.float64), name=\"b\") # INITIALIZE IT WITH RANDOM\n",
        "    \n",
        "    y_hat = tf.add(tf.matmul(x, W), b)\n",
        "    loss = tf.reduce_mean(tf.square(y - y_hat))\n",
        "    \n",
        "    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss) # Try Adam with bigger lr\n",
        "    \n",
        "\n",
        "# CREATE A SESSION TO RUN THE GRAPH\n",
        "with tf.Session(graph=basic_linear_regression_graph) as sess:\n",
        "    \n",
        "    # writer = tf.summary.FileWriter('basic_addition_graph', sess.graph)\n",
        "    # tensorboard --logdir=\"basic_addition_graph\" --port 8000\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    for i in range(epochs): # EPOCHS  1ep is one iteration over complete Dataset.\n",
        "        sess.run(optimizer, feed_dict={x: x_train, y: y_train})\n",
        "        if i % show_every == 0:\n",
        "            cur_loss_train = sess.run(loss, feed_dict={x: x_train, y: y_train})\n",
        "            cur_loss_val = sess.run(loss, feed_dict={x: x_val, y: y_val})\n",
        "            print(\"Training loss at Epoch # \", i, \" is : \", cur_loss_train / y_train.shape[0])\n",
        "            print(\"Validation loss at Epoch # \", i, \" is : \", cur_loss_val / y_val.shape[0])\n",
        "            print('-' * 45)\n",
        "            \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss at Epoch #  0  is :  48.03741334968344\n",
            "Validation loss at Epoch #  0  is :  212.78011937779718\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  100  is :  7.40230072383541\n",
            "Validation loss at Epoch #  100  is :  29.388522941045792\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  200  is :  4.317031230559024\n",
            "Validation loss at Epoch #  200  is :  17.509762604316524\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  300  is :  3.120406108290028\n",
            "Validation loss at Epoch #  300  is :  12.657595844065114\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  400  is :  2.643796208264167\n",
            "Validation loss at Epoch #  400  is :  10.576871754967758\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  500  is :  2.4419441324092457\n",
            "Validation loss at Epoch #  500  is :  9.61624496352105\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  600  is :  2.3451833252496566\n",
            "Validation loss at Epoch #  600  is :  9.122207880215173\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  700  is :  2.2888558687435685\n",
            "Validation loss at Epoch #  700  is :  8.830820677910072\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  800  is :  2.248315557248011\n",
            "Validation loss at Epoch #  800  is :  8.632336762825014\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  900  is :  2.2141621185676423\n",
            "Validation loss at Epoch #  900  is :  8.47929565816006\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  1000  is :  2.182804703775581\n",
            "Validation loss at Epoch #  1000  is :  8.350139383482668\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  1100  is :  2.152866286646178\n",
            "Validation loss at Epoch #  1100  is :  8.234511727522495\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  1200  is :  2.123813624401658\n",
            "Validation loss at Epoch #  1200  is :  8.127142722533064\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  1300  is :  2.0954352240623\n",
            "Validation loss at Epoch #  1300  is :  8.025202842718647\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  1400  is :  2.067642424513058\n",
            "Validation loss at Epoch #  1400  is :  7.92710112704426\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  1500  is :  2.0403935955522754\n",
            "Validation loss at Epoch #  1500  is :  7.831908283809328\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  1600  is :  2.013665248421611\n",
            "Validation loss at Epoch #  1600  is :  7.739063851635689\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  1700  is :  1.9874410212376061\n",
            "Validation loss at Epoch #  1700  is :  7.648219783403031\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  1800  is :  1.961707475513151\n",
            "Validation loss at Epoch #  1800  is :  7.559153316063527\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  1900  is :  1.9364524881190648\n",
            "Validation loss at Epoch #  1900  is :  7.471716863564215\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  2000  is :  1.911664632390377\n",
            "Validation loss at Epoch #  2000  is :  7.385808538189951\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  2100  is :  1.8873329363239413\n",
            "Validation loss at Epoch #  2100  is :  7.301354535588896\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  2200  is :  1.8634467846507174\n",
            "Validation loss at Epoch #  2200  is :  7.218298498190656\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  2300  is :  1.8399958759134616\n",
            "Validation loss at Epoch #  2300  is :  7.136595046722127\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  2400  is :  1.8169702006826365\n",
            "Validation loss at Epoch #  2400  is :  7.056205826527276\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  2500  is :  1.79436002799938\n",
            "Validation loss at Epoch #  2500  is :  6.977097081152754\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  2600  is :  1.772155895121516\n",
            "Validation loss at Epoch #  2600  is :  6.899238157396369\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  2700  is :  1.7503485986898113\n",
            "Validation loss at Epoch #  2700  is :  6.822600580032598\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  2800  is :  1.7289291865926046\n",
            "Validation loss at Epoch #  2800  is :  6.747157475626231\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  2900  is :  1.7078889502484804\n",
            "Validation loss at Epoch #  2900  is :  6.672883210584992\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  3000  is :  1.687219417194844\n",
            "Validation loss at Epoch #  3000  is :  6.599753160879368\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  3100  is :  1.6669123439354494\n",
            "Validation loss at Epoch #  3100  is :  6.52774356281407\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  3200  is :  1.6469597090240633\n",
            "Validation loss at Epoch #  3200  is :  6.45683141380132\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  3300  is :  1.6273537063712586\n",
            "Validation loss at Epoch #  3300  is :  6.3869944040782\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  3400  is :  1.608086738764716\n",
            "Validation loss at Epoch #  3400  is :  6.31821086766437\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  3500  is :  1.5891514115958827\n",
            "Validation loss at Epoch #  3500  is :  6.250459745368926\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  3600  is :  1.570540526785496\n",
            "Validation loss at Epoch #  3600  is :  6.183720555424045\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  3700  is :  1.5522470769014587\n",
            "Validation loss at Epoch #  3700  is :  6.117973369023576\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  3800  is :  1.5342642394632875\n",
            "Validation loss at Epoch #  3800  is :  6.05319878908727\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  3900  is :  1.516585371426407\n",
            "Validation loss at Epoch #  3900  is :  5.989377931213547\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  4000  is :  1.4992040038407513\n",
            "Validation loss at Epoch #  4000  is :  5.926492406176079\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  4100  is :  1.482113836678313\n",
            "Validation loss at Epoch #  4100  is :  5.864524303562498\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  4200  is :  1.4653087338236204\n",
            "Validation loss at Epoch #  4200  is :  5.803456176301253\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  4300  is :  1.4487827182221413\n",
            "Validation loss at Epoch #  4300  is :  5.743271025915075\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  4400  is :  1.4325299671817862\n",
            "Validation loss at Epoch #  4400  is :  5.683952288395212\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  4500  is :  1.4165448078219471\n",
            "Validation loss at Epoch #  4500  is :  5.625483820625713\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  4600  is :  1.4008217126659264\n",
            "Validation loss at Epoch #  4600  is :  5.567849887308769\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  4700  is :  1.3853552953714912\n",
            "Validation loss at Epoch #  4700  is :  5.511035148355303\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  4800  is :  1.3701403065958544\n",
            "Validation loss at Epoch #  4800  is :  5.4550246467134285\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  4900  is :  1.355171629989981\n",
            "Validation loss at Epoch #  4900  is :  5.3998037966132975\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  5000  is :  1.3404442783185448\n",
            "Validation loss at Epoch #  5000  is :  5.345358372209182\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  5100  is :  1.325953389701299\n",
            "Validation loss at Epoch #  5100  is :  5.291674496603518\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  5200  is :  1.3116942239719813\n",
            "Validation loss at Epoch #  5200  is :  5.238738631237565\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  5300  is :  1.297662159150877\n",
            "Validation loss at Epoch #  5300  is :  5.186537565635854\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  5400  is :  1.2838526880276195\n",
            "Validation loss at Epoch #  5400  is :  5.135058407491308\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  5500  is :  1.2702614148503089\n",
            "Validation loss at Epoch #  5500  is :  5.084288573079618\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  5600  is :  1.2568840521178901\n",
            "Validation loss at Epoch #  5600  is :  5.034215777990976\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  5700  is :  1.2437164174721982\n",
            "Validation loss at Epoch #  5700  is :  4.9848280281687005\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  5800  is :  1.2307544306866876\n",
            "Validation loss at Epoch #  5800  is :  4.936113611243996\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  5900  is :  1.217994110748481\n",
            "Validation loss at Epoch #  5900  is :  4.888061088156816\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  6000  is :  1.2054315730309941\n",
            "Validation loss at Epoch #  6000  is :  4.840659285053201\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  6100  is :  1.1930630265540612\n",
            "Validation loss at Epoch #  6100  is :  4.793897285449548\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  6200  is :  1.180884771328832\n",
            "Validation loss at Epoch #  6200  is :  4.747764422654857\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  6300  is :  1.1688931957846238\n",
            "Validation loss at Epoch #  6300  is :  4.702250272442122\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  6400  is :  1.1570847742752652\n",
            "Validation loss at Epoch #  6400  is :  4.65734464596054\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  6500  is :  1.1454560646621703\n",
            "Validation loss at Epoch #  6500  is :  4.613037582880285\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  6600  is :  1.1340037059718784\n",
            "Validation loss at Epoch #  6600  is :  4.569319344761891\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  6700  is :  1.1227244161256447\n",
            "Validation loss at Epoch #  6700  is :  4.52618040864281\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  6800  is :  1.111614989738539\n",
            "Validation loss at Epoch #  6800  is :  4.483611460833679\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  6900  is :  1.100672295986158\n",
            "Validation loss at Epoch #  6900  is :  4.441603390917186\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  7000  is :  1.089893276536677\n",
            "Validation loss at Epoch #  7000  is :  4.400147285942674\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  7100  is :  1.0792749435459736\n",
            "Validation loss at Epoch #  7100  is :  4.359234424809893\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  7200  is :  1.0688143777141066\n",
            "Validation loss at Epoch #  7200  is :  4.318856272835469\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  7300  is :  1.0585087264009452\n",
            "Validation loss at Epoch #  7300  is :  4.279004476495934\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  7400  is :  1.048355201799162\n",
            "Validation loss at Epoch #  7400  is :  4.239670858341151\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  7500  is :  1.0383510791628194\n",
            "Validation loss at Epoch #  7500  is :  4.2008474120727595\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  7600  is :  1.0284936950896795\n",
            "Validation loss at Epoch #  7600  is :  4.162526297781629\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  7700  is :  1.0187804458555574\n",
            "Validation loss at Epoch #  7700  is :  4.124699837339158\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  7800  is :  1.0092087857990608\n",
            "Validation loss at Epoch #  7800  is :  4.087360509937255\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  7900  is :  0.999776225755121\n",
            "Validation loss at Epoch #  7900  is :  4.05050094777179\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  8000  is :  0.9904803315358142\n",
            "Validation loss at Epoch #  8000  is :  4.014113931864851\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  8100  is :  0.9813187224568418\n",
            "Validation loss at Epoch #  8100  is :  3.9781923880209864\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  8200  is :  0.9722890699083727\n",
            "Validation loss at Epoch #  8200  is :  3.9427293829129324\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  8300  is :  0.9633890959687221\n",
            "Validation loss at Epoch #  8300  is :  3.9077181202925226\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  8400  is :  0.9546165720595619\n",
            "Validation loss at Epoch #  8400  is :  3.873151937322367\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  8500  is :  0.945969317641366\n",
            "Validation loss at Epoch #  8500  is :  3.8390243010245397\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  8600  is :  0.9374451989477575\n",
            "Validation loss at Epoch #  8600  is :  3.8053288048418623\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  8700  is :  0.9290421277576254\n",
            "Validation loss at Epoch #  8700  is :  3.772059165308513\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  8800  is :  0.9207580602036434\n",
            "Validation loss at Epoch #  8800  is :  3.7392092188257653\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  8900  is :  0.912590995616305\n",
            "Validation loss at Epoch #  8900  is :  3.706772918539761\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  9000  is :  0.904538975402066\n",
            "Validation loss at Epoch #  9000  is :  3.6747443313174744\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  9100  is :  0.8966000819547388\n",
            "Validation loss at Epoch #  9100  is :  3.6431176348178638\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  9200  is :  0.888772437598973\n",
            "Validation loss at Epoch #  9200  is :  3.6118871146548663\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  9300  is :  0.8810542035649127\n",
            "Validation loss at Epoch #  9300  is :  3.5810471616491277\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  9400  is :  0.873443578992849\n",
            "Validation loss at Epoch #  9400  is :  3.5505922691655027\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  9500  is :  0.8659387999672571\n",
            "Validation loss at Epoch #  9500  is :  3.5205170305335374\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  9600  is :  0.85853813857895\n",
            "Validation loss at Epoch #  9600  is :  3.4908161365479473\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  9700  is :  0.851239902014662\n",
            "Validation loss at Epoch #  9700  is :  3.46148437304657\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  9800  is :  0.8440424316732157\n",
            "Validation loss at Epoch #  9800  is :  3.432516618563124\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  9900  is :  0.8369441023073514\n",
            "Validation loss at Epoch #  9900  is :  3.4039078420522584\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vO18TxdSLnIH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Define Custom loss with Condition**\n",
        "**Huber Loss**\n",
        "\n",
        "A little Intuituin behind it:\n",
        "\n",
        "If the distance between predicted and original is lesser than some delta use the squared loss, otherwise use the absolute difference to cater for the outliers.\n",
        "\n",
        "Let's see how we can make a conditional statement like this in tensorflow.\n",
        "\n",
        "\\begin{equation*}\n",
        "L_\\delta (y,f(x)) = \\begin{vmatrix}\n",
        "1/2(y-f(x))^2 for |y - f(x)| \\leq \\delta, \\\\\n",
        "\\delta|y-f(x)| - 1/2\\delta^2 Otherwise\\\\\n",
        "\\end{vmatrix}\n",
        "\\end{equation*}"
      ]
    },
    {
      "metadata": {
        "id": "kVdY0J1oJlim",
        "colab_type": "code",
        "outputId": "be30108b-89e1-434f-85b4-aeb3945373d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7217
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Training Over Boston Housing. Its a simple Linear regression with conditional Loss\n",
        "'''\n",
        "\n",
        "# LOAD DATASET\n",
        "dataset = tf.keras.datasets.boston_housing\n",
        "(x_train, y_train), (x_val, y_val) = dataset.load_data()\n",
        "\n",
        "# CONFIGURE\n",
        "lr = 1e-7\n",
        "show_every = 100\n",
        "epochs = 10000\n",
        "\n",
        "# CREATE GRAPH\n",
        "basic_linear_regression_graph = tf.Graph()\n",
        "with basic_linear_regression_graph.as_default():\n",
        "    \n",
        "    def huber_loss(labels, predicted, delta=1.0):\n",
        "        tf_delta = tf.constant(value=delta, dtype=tf.float64)\n",
        "        residual = tf.reduce_mean(tf.abs(predicted - labels))\n",
        "        condition = tf.less(residual, tf_delta)\n",
        "        small_res = 0.5 * tf.square(residual)\n",
        "        large_res = tf_delta * residual - 0.5 * tf.square(tf_delta)\n",
        "        return tf.where(condition, small_res, large_res)\n",
        "        \n",
        "    \n",
        "    x = tf.placeholder(name=\"x\",\n",
        "                    dtype=tf.float64)\n",
        "    \n",
        "    y = tf.placeholder(name=\"y\",\n",
        "                    dtype=tf.float64)\n",
        "    \n",
        "    W = tf.Variable(tf.truncated_normal([13, 1], mean=0.0, stddev=1.0, dtype=tf.float64), name=\"W\") # INITIALIZE IT WITH RANDOM\n",
        "    b = tf.Variable(tf.zeros(1, dtype = tf.float64), name=\"b\") # INITIALIZE IT WITH RANDOM\n",
        "    \n",
        "    y_hat = tf.add(tf.matmul(x, W), b)\n",
        "    loss = huber_loss(y, y_hat, delta=1.0)\n",
        "#     print(loss)\n",
        "    \n",
        "    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss) # Try Adam with bigger lr\n",
        "    \n",
        "\n",
        "# CREATE A SESSION TO RUN THE GRAPH\n",
        "with tf.Session(graph=basic_linear_regression_graph) as sess:\n",
        "    \n",
        "    # writer = tf.summary.FileWriter('basic_addition_graph', sess.graph)\n",
        "    # tensorboard --logdir=\"basic_addition_graph\" --port 8000\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    for i in range(epochs): # EPOCHS  1ep is one iteration over complete Dataset.\n",
        "        sess.run(optimizer, feed_dict={x: x_train, y: y_train})\n",
        "        if i % show_every == 0:\n",
        "            cur_loss_train = sess.run(loss, feed_dict={x: x_train, y: y_train})\n",
        "            cur_loss_val = sess.run(loss, feed_dict={x: x_val, y: y_val})\n",
        "            print(\"Training loss at Epoch # \", i, \" is : \", cur_loss_train / y_train.shape[0])\n",
        "            print(\"Validation loss at Epoch # \", i, \" is : \", cur_loss_val / y_val.shape[0])\n",
        "            print('-' * 45)\n",
        "            \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "()\n",
            "Training loss at Epoch #  0  is :  1.2728749119818021\n",
            "Validation loss at Epoch #  0  is :  5.242834525885368\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  100  is :  1.2655406620238956\n",
            "Validation loss at Epoch #  100  is :  5.213012623527477\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  200  is :  1.2582064120659013\n",
            "Validation loss at Epoch #  200  is :  5.183190721169599\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  300  is :  1.250872162107964\n",
            "Validation loss at Epoch #  300  is :  5.15336881881174\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  400  is :  1.2435379121500505\n",
            "Validation loss at Epoch #  400  is :  5.123546916453859\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  500  is :  1.2362036621920893\n",
            "Validation loss at Epoch #  500  is :  5.093725014095966\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  600  is :  1.2288694122341275\n",
            "Validation loss at Epoch #  600  is :  5.063903111738116\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  700  is :  1.2215351622761743\n",
            "Validation loss at Epoch #  700  is :  5.034081209380246\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  800  is :  1.2142009123182562\n",
            "Validation loss at Epoch #  800  is :  5.004259307022363\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  900  is :  1.2068666623602797\n",
            "Validation loss at Epoch #  900  is :  4.974437404664489\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  1000  is :  1.1995324124023423\n",
            "Validation loss at Epoch #  1000  is :  4.944615502306603\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  1100  is :  1.1921981624443803\n",
            "Validation loss at Epoch #  1100  is :  4.914793599948746\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  1200  is :  1.184863912486432\n",
            "Validation loss at Epoch #  1200  is :  4.884971697590863\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  1300  is :  1.1775296625284593\n",
            "Validation loss at Epoch #  1300  is :  4.8551497952329825\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  1400  is :  1.1701954125705378\n",
            "Validation loss at Epoch #  1400  is :  4.825327892875139\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  1500  is :  1.1628611626125724\n",
            "Validation loss at Epoch #  1500  is :  4.795505990517216\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  1600  is :  1.1555269126546608\n",
            "Validation loss at Epoch #  1600  is :  4.7656840881593645\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  1700  is :  1.1481926626966519\n",
            "Validation loss at Epoch #  1700  is :  4.735862185801489\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  1800  is :  1.1408584127387231\n",
            "Validation loss at Epoch #  1800  is :  4.706040283443622\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  1900  is :  1.1335241627807762\n",
            "Validation loss at Epoch #  1900  is :  4.676218381085751\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  2000  is :  1.126189912822809\n",
            "Validation loss at Epoch #  2000  is :  4.646396478727872\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  2100  is :  1.1188556628648834\n",
            "Validation loss at Epoch #  2100  is :  4.616574576369994\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  2200  is :  1.111521412906921\n",
            "Validation loss at Epoch #  2200  is :  4.586752674012118\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  2300  is :  1.1041871629489513\n",
            "Validation loss at Epoch #  2300  is :  4.556930771654245\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  2400  is :  1.0968529129910367\n",
            "Validation loss at Epoch #  2400  is :  4.527108869296373\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  2500  is :  1.0895186630330684\n",
            "Validation loss at Epoch #  2500  is :  4.49728696693848\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  2600  is :  1.0821844130751088\n",
            "Validation loss at Epoch #  2600  is :  4.467465064580614\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  2700  is :  1.074850163117184\n",
            "Validation loss at Epoch #  2700  is :  4.437643162222743\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  2800  is :  1.0675159131592142\n",
            "Validation loss at Epoch #  2800  is :  4.407821259864872\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  2900  is :  1.0601816632012664\n",
            "Validation loss at Epoch #  2900  is :  4.377999357506996\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  3000  is :  1.0528474132433097\n",
            "Validation loss at Epoch #  3000  is :  4.348177455149114\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  3100  is :  1.0455131632853667\n",
            "Validation loss at Epoch #  3100  is :  4.318355552791246\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  3200  is :  1.0381789133273878\n",
            "Validation loss at Epoch #  3200  is :  4.288533650433367\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  3300  is :  1.030844663369453\n",
            "Validation loss at Epoch #  3300  is :  4.258711748075494\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  3400  is :  1.0235105285087553\n",
            "Validation loss at Epoch #  3400  is :  4.2288900789180675\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  3500  is :  1.016177185637597\n",
            "Validation loss at Epoch #  3500  is :  4.19907001214308\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  3600  is :  1.0088466653104693\n",
            "Validation loss at Epoch #  3600  is :  4.169255681301408\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  3700  is :  1.0015228014341049\n",
            "Validation loss at Epoch #  3700  is :  4.139454865850169\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  3800  is :  0.9942120917484647\n",
            "Validation loss at Epoch #  3800  is :  4.109680824558363\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  3900  is :  0.9869224410439026\n",
            "Validation loss at Epoch #  3900  is :  4.079949748902045\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  4000  is :  0.9796653344991181\n",
            "Validation loss at Epoch #  4000  is :  4.050285285720799\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  4100  is :  0.9724521577566329\n",
            "Validation loss at Epoch #  4100  is :  4.020711192420627\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  4200  is :  0.9652869273642031\n",
            "Validation loss at Epoch #  4200  is :  3.991236292696073\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  4300  is :  0.9581736331349311\n",
            "Validation loss at Epoch #  4300  is :  3.9618695574272613\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  4400  is :  0.9511123940303299\n",
            "Validation loss at Epoch #  4400  is :  3.9326220452578924\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  4500  is :  0.944100520754588\n",
            "Validation loss at Epoch #  4500  is :  3.9035058869742527\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  4600  is :  0.9371404204541529\n",
            "Validation loss at Epoch #  4600  is :  3.874518579757912\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  4700  is :  0.9302252091346668\n",
            "Validation loss at Epoch #  4700  is :  3.8456556384215483\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  4800  is :  0.923358267092488\n",
            "Validation loss at Epoch #  4800  is :  3.816925861117285\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  4900  is :  0.9165417754973588\n",
            "Validation loss at Epoch #  4900  is :  3.7883781337019733\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  5000  is :  0.9097851446333992\n",
            "Validation loss at Epoch #  5000  is :  3.76006559474857\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  5100  is :  0.9030916383971874\n",
            "Validation loss at Epoch #  5100  is :  3.7319587221325055\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  5200  is :  0.896461429971053\n",
            "Validation loss at Epoch #  5200  is :  3.7040312059344562\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  5300  is :  0.8898913639891592\n",
            "Validation loss at Epoch #  5300  is :  3.676264244352934\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  5400  is :  0.8833772545012587\n",
            "Validation loss at Epoch #  5400  is :  3.648639926226158\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  5500  is :  0.8769115997984198\n",
            "Validation loss at Epoch #  5500  is :  3.6211456405462963\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  5600  is :  0.8704854499992086\n",
            "Validation loss at Epoch #  5600  is :  3.593743392259191\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  5700  is :  0.864091848740875\n",
            "Validation loss at Epoch #  5700  is :  3.5664139842672977\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  5800  is :  0.8577252729956198\n",
            "Validation loss at Epoch #  5800  is :  3.539158432146511\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  5900  is :  0.8513848345407333\n",
            "Validation loss at Epoch #  5900  is :  3.511990723820722\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  6000  is :  0.845063517552964\n",
            "Validation loss at Epoch #  6000  is :  3.4849025314250768\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  6100  is :  0.8387619592332916\n",
            "Validation loss at Epoch #  6100  is :  3.4578949749042898\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  6200  is :  0.832483779977476\n",
            "Validation loss at Epoch #  6200  is :  3.4309744872130112\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  6300  is :  0.8262281025863782\n",
            "Validation loss at Epoch #  6300  is :  3.404146360023134\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  6400  is :  0.8199911665394991\n",
            "Validation loss at Epoch #  6400  is :  3.3774375343790006\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  6500  is :  0.8137712936316313\n",
            "Validation loss at Epoch #  6500  is :  3.35088822058926\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  6600  is :  0.8075683305626022\n",
            "Validation loss at Epoch #  6600  is :  3.3244981287110527\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  6700  is :  0.8013778484739481\n",
            "Validation loss at Epoch #  6700  is :  3.2982377634001128\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  6800  is :  0.7951984676136686\n",
            "Validation loss at Epoch #  6800  is :  3.2720890189520695\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  6900  is :  0.7890331283135529\n",
            "Validation loss at Epoch #  6900  is :  3.2460735924802995\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  7000  is :  0.7828840059237999\n",
            "Validation loss at Epoch #  7000  is :  3.2201809759088804\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  7100  is :  0.7767549555034761\n",
            "Validation loss at Epoch #  7100  is :  3.1944565188560894\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  7200  is :  0.7706470049379482\n",
            "Validation loss at Epoch #  7200  is :  3.168902313260488\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  7300  is :  0.7645596066105795\n",
            "Validation loss at Epoch #  7300  is :  3.1435000723134867\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  7400  is :  0.7584914464231841\n",
            "Validation loss at Epoch #  7400  is :  3.1182838845273575\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  7500  is :  0.7524395967306481\n",
            "Validation loss at Epoch #  7500  is :  3.0932277343841386\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  7600  is :  0.7464053907890021\n",
            "Validation loss at Epoch #  7600  is :  3.0682979633963114\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  7700  is :  0.7403948925837315\n",
            "Validation loss at Epoch #  7700  is :  3.0434784455698902\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  7800  is :  0.7344094071546391\n",
            "Validation loss at Epoch #  7800  is :  3.0187428044540074\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  7900  is :  0.7284559740956535\n",
            "Validation loss at Epoch #  7900  is :  2.9940959593712075\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  8000  is :  0.7225373562014926\n",
            "Validation loss at Epoch #  8000  is :  2.9695462750948924\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  8100  is :  0.7166534902937424\n",
            "Validation loss at Epoch #  8100  is :  2.9450799198414908\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  8200  is :  0.7108013626958624\n",
            "Validation loss at Epoch #  8200  is :  2.920687223851971\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  8300  is :  0.7049853343231366\n",
            "Validation loss at Epoch #  8300  is :  2.8963868731370734\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  8400  is :  0.6992021778180135\n",
            "Validation loss at Epoch #  8400  is :  2.8721695693499383\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  8500  is :  0.6934552195167558\n",
            "Validation loss at Epoch #  8500  is :  2.848045522437406\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  8600  is :  0.6877438511878615\n",
            "Validation loss at Epoch #  8600  is :  2.82400696628756\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  8700  is :  0.6820615014313993\n",
            "Validation loss at Epoch #  8700  is :  2.8000314237239996\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  8800  is :  0.6764089127985325\n",
            "Validation loss at Epoch #  8800  is :  2.7761206661355375\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  8900  is :  0.6707916786014284\n",
            "Validation loss at Epoch #  8900  is :  2.7522872764570776\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  9000  is :  0.6652101216446591\n",
            "Validation loss at Epoch #  9000  is :  2.728532402446657\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  9100  is :  0.6596617905724476\n",
            "Validation loss at Epoch #  9100  is :  2.7048509971187635\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  9200  is :  0.6541373006112255\n",
            "Validation loss at Epoch #  9200  is :  2.681222612665014\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  9300  is :  0.6486305910145413\n",
            "Validation loss at Epoch #  9300  is :  2.6576338842417546\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  9400  is :  0.6431378839567079\n",
            "Validation loss at Epoch #  9400  is :  2.634076395257123\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  9500  is :  0.6376593014304681\n",
            "Validation loss at Epoch #  9500  is :  2.61055046146546\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  9600  is :  0.6321951571273434\n",
            "Validation loss at Epoch #  9600  is :  2.5870568637523674\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  9700  is :  0.6267453715023291\n",
            "Validation loss at Epoch #  9700  is :  2.5635954925979783\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  9800  is :  0.6213090344371256\n",
            "Validation loss at Epoch #  9800  is :  2.5401643601696433\n",
            "---------------------------------------------\n",
            "()\n",
            "Training loss at Epoch #  9900  is :  0.6158846996952968\n",
            "Validation loss at Epoch #  9900  is :  2.5167602811615715\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hBQktV3slSl2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Vanilla Classification with Logistic Regression**"
      ]
    },
    {
      "metadata": {
        "id": "gq0dpUgllmPb",
        "colab_type": "code",
        "outputId": "f054dfe8-4ee8-446a-caf2-b1d4aa4cf828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1169
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Training Over MNIST \n",
        "'''\n",
        "\n",
        "# LOAD DATASET\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "\n",
        "# CONFIGURE\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "epochs = 30\n",
        "\n",
        "# CREATE GRAPH\n",
        "basic_logistic_regression_graph = tf.Graph()\n",
        "with basic_logistic_regression_graph.as_default():\n",
        "    \n",
        "    x = tf.placeholder(name=\"x\",\n",
        "                    shape=(None, 784),\n",
        "                    dtype=tf.float32)\n",
        "    \n",
        "    y = tf.placeholder(name=\"y\",\n",
        "                    shape=(None, 10),\n",
        "                    dtype=tf.float32)\n",
        "    \n",
        "    W = tf.Variable(tf.truncated_normal([ 784, 10], mean=0.0, stddev=1.0, dtype=tf.float32), name=\"W\")\n",
        "    b = tf.Variable(tf.zeros(10, dtype = tf.float32), name=\"b\") \n",
        "    \n",
        "    logits = tf.nn.relu(tf.add(tf.matmul(x, W), b))\n",
        "    \n",
        "    loss_for_each_sample = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
        "    \n",
        "    loss = tf.reduce_mean(loss_for_each_sample)\n",
        "    \n",
        "    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss) # Try Adam with bigger lr\n",
        "    \n",
        "\n",
        "# CREATE A SESSION TO RUN THE GRAPH\n",
        "with tf.Session(graph=basic_logistic_regression_graph) as sess:\n",
        "    \n",
        "    # writer = tf.summary.FileWriter('basic_addition_graph', sess.graph)\n",
        "    # tensorboard --logdir=\"basic_addition_graph\" --port 8000\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    for i in range(epochs): # EPOCHS  1ep is one iteration over complete Dataset.\n",
        "        \n",
        "        n_batches = int(mnist.train.num_examples / batch_size)\n",
        "        total_loss = 0\n",
        "        for j in range(n_batches):\n",
        "            \n",
        "            x_batch , y_batch = mnist.train.next_batch(batch_size)\n",
        "            _, cur_loss_train = sess.run([optimizer, loss], feed_dict={x: x_batch, y: y_batch})\n",
        "            total_loss = total_loss + cur_loss_train\n",
        "            \n",
        "\n",
        "        print(\"Training loss at Epoch # \", i, \" is : \", total_loss / n_batches)\n",
        "        print('-' * 45)\n",
        "            \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Training loss at Epoch #  0  is :  6.432287242862728\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  1  is :  5.089530738083632\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  2  is :  4.314753842520547\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  3  is :  3.8152912418881217\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  4  is :  3.478019020774148\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  5  is :  3.235242928658332\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  6  is :  3.061584578805314\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  7  is :  2.926469368256611\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  8  is :  2.825787796562924\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  9  is :  2.74276505483614\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  10  is :  2.678474748607004\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  11  is :  2.6246917370038156\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  12  is :  2.578315763206749\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  13  is :  2.5448736549813153\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  14  is :  2.5107848622026423\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  15  is :  2.480887064566979\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  16  is :  2.458981876884585\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  17  is :  2.4378657229852565\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  18  is :  2.418341995674969\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  19  is :  2.3979631298349733\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  20  is :  2.385019544677023\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  21  is :  2.3716425506662935\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  22  is :  2.359316699154727\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  23  is :  2.346666605600388\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  24  is :  2.335593026279014\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  25  is :  2.3272978059061757\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  26  is :  2.3159925626430202\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  27  is :  2.3073043423099118\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  28  is :  2.296801938877239\n",
            "---------------------------------------------\n",
            "Training loss at Epoch #  29  is :  2.2940437554757356\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AcjI988f01YJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **One Hidden Layer...**"
      ]
    },
    {
      "metadata": {
        "id": "AkzAFZ3guqOZ",
        "colab_type": "code",
        "outputId": "c34a6283-2f01-465a-ca41-6311a5f3da43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1709
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Training Over MNIST\n",
        "'''\n",
        "\n",
        "# LOAD DATASET\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# CONFIGURE\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "epochs = 30\n",
        "\n",
        "# CREATE GRAPH\n",
        "basic_logistic_regression_graph = tf.Graph()\n",
        "with basic_logistic_regression_graph.as_default():\n",
        "    \n",
        "    with tf.name_scope(\"placeholders\"):\n",
        "        x = tf.placeholder(name=\"x\",\n",
        "                        shape=(None, 784),\n",
        "                        dtype=tf.float32)\n",
        "        y = tf.placeholder(name=\"y\",\n",
        "                        shape=(None, 10),\n",
        "                        dtype=tf.float32)\n",
        "    \n",
        "    with tf.name_scope(\"Hidden-Layer\"):\n",
        "        W1 = tf.Variable(tf.truncated_normal([ 784, 128], mean=0.0, stddev=1.0, dtype=tf.float32), name=\"W1\")\n",
        "        b1 = tf.Variable(tf.zeros(128, dtype = tf.float32), name=\"b1\")\n",
        "        h1 = tf.nn.relu(tf.add(tf.matmul(x, W1), b1))\n",
        "        \n",
        "    with tf.name_scope(\"Output-Layer\"):\n",
        "        W2 = tf.Variable(tf.truncated_normal([ 128, 10], mean=0.0, stddev=1.0, dtype=tf.float32), name=\"W2\")\n",
        "        b2 = tf.Variable(tf.zeros(10, dtype = tf.float32), name=\"b2\")        \n",
        "        logits = tf.add(tf.matmul(h1, W2), b2)\n",
        "   \n",
        "    \n",
        "    loss_for_each_sample = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y) \n",
        "    loss = tf.reduce_mean(loss_for_each_sample)\n",
        "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss) # Try Adam with bigger lr\n",
        "        \n",
        "# CREATE A SESSION TO RUN THE GRAPH\n",
        "with tf.Session(graph=basic_logistic_regression_graph) as sess:\n",
        "    \n",
        "    # writer = tf.summary.FileWriter('basic_addition_graph', sess.graph)\n",
        "    # tensorboard --logdir=\"basic_addition_graph\" --port 8000\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    for i in range(epochs): # EPOCHS  1ep is one iteration over complete Dataset.\n",
        "        \n",
        "        n_batches = int(mnist.train.num_examples / batch_size)\n",
        "        total_loss = 0\n",
        "        for j in range(n_batches):\n",
        "            x_batch , y_batch = mnist.train.next_batch(batch_size)\n",
        "            _, cur_loss_train = sess.run([optimizer, loss], feed_dict={x: x_batch, y: y_batch})\n",
        "            total_loss = total_loss + cur_loss_train\n",
        "            _, cur_loss_train = sess.run([optimizer, loss], feed_dict={x: x_batch, y: y_batch})\n",
        "        \n",
        "        print(\"Training loss at Epoch #\", i, \" is :\", total_loss / n_batches)\n",
        "        \n",
        "        n_batches = int(mnist.test.num_examples / batch_size)\n",
        "        total_acc = 0.0\n",
        "        for j in range(n_batches):\n",
        "            x_batch , y_batch = mnist.test.next_batch(batch_size)\n",
        "            preds = sess.run(logits, feed_dict={x: x_batch, y: y_batch})            \n",
        "            correct_preds = np.zeros_like(preds)\n",
        "            correct_preds[np.arange(len(preds)), preds.argmax(1)] = 1\n",
        "            acc = metrics.accuracy_score(y_true=y_batch.astype(int), y_pred=correct_preds.astype(int))\n",
        "            total_acc = total_acc + acc\n",
        "            \n",
        "            \n",
        "        print(\"Validation Accuracy at Epoch #\", i, \" is :\", total_acc / n_batches)\n",
        "        print('-' * 45)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Training loss at Epoch # 0  is : 8.5682506783581\n",
            "Validation Accuracy at Epoch # 0  is : 0.8535657051282052\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 1  is : 2.200333347756824\n",
            "Validation Accuracy at Epoch # 1  is : 0.8894230769230769\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 2  is : 1.3598866055617522\n",
            "Validation Accuracy at Epoch # 2  is : 0.9076522435897436\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 3  is : 0.9645724134011702\n",
            "Validation Accuracy at Epoch # 3  is : 0.9220753205128205\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 4  is : 0.7154718012242884\n",
            "Validation Accuracy at Epoch # 4  is : 0.9322916666666666\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 5  is : 0.5415254692049883\n",
            "Validation Accuracy at Epoch # 5  is : 0.9342948717948718\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 6  is : 0.41664634914504306\n",
            "Validation Accuracy at Epoch # 6  is : 0.9394030448717948\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 7  is : 0.3287415563786676\n",
            "Validation Accuracy at Epoch # 7  is : 0.9382011217948718\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 8  is : 0.25436479364326964\n",
            "Validation Accuracy at Epoch # 8  is : 0.9444110576923077\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 9  is : 0.20433941133118708\n",
            "Validation Accuracy at Epoch # 9  is : 0.9430088141025641\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 10  is : 0.16378538077005178\n",
            "Validation Accuracy at Epoch # 10  is : 0.9482171474358975\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 11  is : 0.1274459919286295\n",
            "Validation Accuracy at Epoch # 11  is : 0.9506209935897436\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 12  is : 0.1054203858854692\n",
            "Validation Accuracy at Epoch # 12  is : 0.9537259615384616\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 13  is : 0.08090553898198095\n",
            "Validation Accuracy at Epoch # 13  is : 0.9529246794871795\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 14  is : 0.06643423042924954\n",
            "Validation Accuracy at Epoch # 14  is : 0.9517227564102564\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 15  is : 0.05244458867824476\n",
            "Validation Accuracy at Epoch # 15  is : 0.9510216346153846\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 16  is : 0.0451955290183985\n",
            "Validation Accuracy at Epoch # 16  is : 0.9554286858974359\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 17  is : 0.03737060786778146\n",
            "Validation Accuracy at Epoch # 17  is : 0.9529246794871795\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 18  is : 0.033031720312878446\n",
            "Validation Accuracy at Epoch # 18  is : 0.9565304487179487\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 19  is : 0.02884623539852124\n",
            "Validation Accuracy at Epoch # 19  is : 0.9560296474358975\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 20  is : 0.01971274233169753\n",
            "Validation Accuracy at Epoch # 20  is : 0.9573317307692307\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 21  is : 0.020312487096943967\n",
            "Validation Accuracy at Epoch # 21  is : 0.957832532051282\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 22  is : 0.014932512046131078\n",
            "Validation Accuracy at Epoch # 22  is : 0.9592347756410257\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 23  is : 0.01657253567981992\n",
            "Validation Accuracy at Epoch # 23  is : 0.9576322115384616\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 24  is : 0.01600072074589579\n",
            "Validation Accuracy at Epoch # 24  is : 0.9594350961538461\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 25  is : 0.009659571307782028\n",
            "Validation Accuracy at Epoch # 25  is : 0.9582331730769231\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 26  is : 0.010973124288844142\n",
            "Validation Accuracy at Epoch # 26  is : 0.9603365384615384\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 27  is : 0.012445747286734747\n",
            "Validation Accuracy at Epoch # 27  is : 0.9600360576923077\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 28  is : 0.008287559885076434\n",
            "Validation Accuracy at Epoch # 28  is : 0.9623397435897436\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 29  is : 0.006551211245261948\n",
            "Validation Accuracy at Epoch # 29  is : 0.960136217948718\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "grBVaFFNEfcP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Stack More Layers....**![alt text](https://i.imgur.com/B9KgNW7.png)"
      ]
    },
    {
      "metadata": {
        "id": "2ZqvfHDj3vbV",
        "colab_type": "code",
        "outputId": "0343da83-78a1-45fe-9f2d-a453267b0ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1709
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Training Over MNIST\n",
        "'''\n",
        "\n",
        "# LOAD DATASET\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# CONFIGURE\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "epochs = 30\n",
        "\n",
        "# CREATE GRAPH\n",
        "basic_logistic_regression_graph = tf.Graph()\n",
        "with basic_logistic_regression_graph.as_default():\n",
        "    \n",
        "    def get_dense_hidden_layer(x_temp, in_shape=0, out_shape=0):\n",
        "        W_temp = tf.Variable(tf.truncated_normal([in_shape, out_shape], mean=0.0, stddev=1.0, dtype=tf.float32))\n",
        "        b_temp = tf.Variable(tf.zeros(out_shape, dtype = tf.float32))\n",
        "        return tf.nn.relu(tf.add(tf.matmul(x_temp, W_temp), b_temp))\n",
        "    \n",
        "    with tf.name_scope(\"placeholders\"):\n",
        "        x = tf.placeholder(name=\"x\",\n",
        "                        shape=(None, 784),\n",
        "                        dtype=tf.float32)\n",
        "        y = tf.placeholder(name=\"y\",\n",
        "                        shape=(None, 10),\n",
        "                        dtype=tf.float32)\n",
        "    \n",
        "    with tf.name_scope(\"Hidden-Layers\"):\n",
        "        h1 = get_dense_hidden_layer(x, in_shape=784, out_shape=512)\n",
        "        h2 = get_dense_hidden_layer(h1, in_shape=512, out_shape=256)\n",
        "        #h2 = tf.nn.dropout(h2)\n",
        "        h3 = get_dense_hidden_layer(h2, in_shape=256, out_shape=128)\n",
        "  \n",
        "    with tf.name_scope(\"Output-Layer\"):\n",
        "        W = tf.Variable(tf.truncated_normal([ 128, 10], mean=0.0, stddev=1.0, dtype=tf.float32), name=\"W\")\n",
        "        b = tf.Variable(tf.zeros(10, dtype = tf.float32), name=\"b\")        \n",
        "        logits = tf.add(tf.matmul(h3, W), b)\n",
        "   \n",
        "    loss_for_each_sample = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y) \n",
        "    loss = tf.reduce_mean(loss_for_each_sample)\n",
        "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss) # Try Adam with bigger lr\n",
        "\n",
        "    \n",
        "    \n",
        "# CREATE A SESSION TO RUN THE GRAPH\n",
        "with tf.Session(graph=basic_logistic_regression_graph) as sess:\n",
        "    \n",
        "    # writer = tf.summary.FileWriter('basic_addition_graph', sess.graph)\n",
        "    # tensorboard --logdir=\"basic_addition_graph\" --port 8000\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    for i in range(epochs): # EPOCHS  1ep is one iteration over complete Dataset.\n",
        "        \n",
        "        n_batches = int(mnist.train.num_examples / batch_size)\n",
        "        total_loss = 0\n",
        "        for j in range(n_batches):\n",
        "            x_batch , y_batch = mnist.train.next_batch(batch_size)\n",
        "            _, cur_loss_train = sess.run([optimizer, loss], feed_dict={x: x_batch, y: y_batch})\n",
        "            total_loss = total_loss + cur_loss_train\n",
        "            _, cur_loss_train = sess.run([optimizer, loss], feed_dict={x: x_batch, y: y_batch})\n",
        "        \n",
        "        print(\"Training loss at Epoch #\", i, \" is :\", total_loss / n_batches)\n",
        "        \n",
        "        n_batches = int(mnist.test.num_examples / batch_size)\n",
        "        total_acc = 0.0\n",
        "        for j in range(n_batches):\n",
        "            x_batch , y_batch = mnist.test.next_batch(batch_size)\n",
        "            preds = sess.run(logits, feed_dict={x: x_batch, y: y_batch})            \n",
        "            correct_preds = np.zeros_like(preds)\n",
        "            correct_preds[np.arange(len(preds)), preds.argmax(1)] = 1\n",
        "            acc = metrics.accuracy_score(y_true=y_batch.astype(int), y_pred=correct_preds.astype(int))\n",
        "            total_acc = total_acc + acc\n",
        "            \n",
        "            \n",
        "        print(\"Validation Accuracy at Epoch #\", i, \" is :\", total_acc / n_batches)\n",
        "        print('-' * 45)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Training loss at Epoch # 0  is : 529.9906665739758\n",
            "Validation Accuracy at Epoch # 0  is : 0.8935296474358975\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 1  is : 107.69994796183838\n",
            "Validation Accuracy at Epoch # 1  is : 0.9177684294871795\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 2  is : 57.79197221952718\n",
            "Validation Accuracy at Epoch # 2  is : 0.9322916666666666\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 3  is : 33.41335638586458\n",
            "Validation Accuracy at Epoch # 3  is : 0.9341947115384616\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 4  is : 22.377287357630934\n",
            "Validation Accuracy at Epoch # 4  is : 0.9421073717948718\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 5  is : 15.87362136846612\n",
            "Validation Accuracy at Epoch # 5  is : 0.9423076923076923\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 6  is : 10.902264386751389\n",
            "Validation Accuracy at Epoch # 6  is : 0.946113782051282\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 7  is : 9.328942480049243\n",
            "Validation Accuracy at Epoch # 7  is : 0.9514222756410257\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 8  is : 7.589783291952915\n",
            "Validation Accuracy at Epoch # 8  is : 0.9548277243589743\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 9  is : 6.761714237307943\n",
            "Validation Accuracy at Epoch # 9  is : 0.9541266025641025\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 10  is : 7.34676369730608\n",
            "Validation Accuracy at Epoch # 10  is : 0.9557291666666666\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 11  is : 5.393221142014104\n",
            "Validation Accuracy at Epoch # 11  is : 0.9581330128205128\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 12  is : 4.9213996689062895\n",
            "Validation Accuracy at Epoch # 12  is : 0.9585336538461539\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 13  is : 4.764971286326598\n",
            "Validation Accuracy at Epoch # 13  is : 0.9627403846153846\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 14  is : 4.190814009889214\n",
            "Validation Accuracy at Epoch # 14  is : 0.9612379807692307\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 15  is : 4.392534917770117\n",
            "Validation Accuracy at Epoch # 15  is : 0.9573317307692307\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 16  is : 4.012045186890211\n",
            "Validation Accuracy at Epoch # 16  is : 0.9582331730769231\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 17  is : 3.1788439885997626\n",
            "Validation Accuracy at Epoch # 17  is : 0.9620392628205128\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 18  is : 4.22160513669061\n",
            "Validation Accuracy at Epoch # 18  is : 0.9555288461538461\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 19  is : 3.639023281801308\n",
            "Validation Accuracy at Epoch # 19  is : 0.9647435897435898\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 20  is : 2.4743435949909247\n",
            "Validation Accuracy at Epoch # 20  is : 0.9667467948717948\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 21  is : 2.9275652796068665\n",
            "Validation Accuracy at Epoch # 21  is : 0.9636418269230769\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 22  is : 3.4184566585910474\n",
            "Validation Accuracy at Epoch # 22  is : 0.9641426282051282\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 23  is : 3.42636148359987\n",
            "Validation Accuracy at Epoch # 23  is : 0.9655448717948718\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 24  is : 2.308988012979597\n",
            "Validation Accuracy at Epoch # 24  is : 0.9657451923076923\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 25  is : 2.5241093008359705\n",
            "Validation Accuracy at Epoch # 25  is : 0.9675480769230769\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 26  is : 2.1507274229003936\n",
            "Validation Accuracy at Epoch # 26  is : 0.9633413461538461\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 27  is : 1.3669169088375155\n",
            "Validation Accuracy at Epoch # 27  is : 0.9671474358974359\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 28  is : 2.463110717438563\n",
            "Validation Accuracy at Epoch # 28  is : 0.9612379807692307\n",
            "---------------------------------------------\n",
            "Training loss at Epoch # 29  is : 2.616697985590898\n",
            "Validation Accuracy at Epoch # 29  is : 0.9686498397435898\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ESovQVGCLqFh",
        "colab_type": "code",
        "outputId": "c58210ad-f687-4ab9-c1c2-f7a5d350033b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1171875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "CncZRJlwRQxL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}